import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import count, sum, year, to_date
## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
customer_df = spark.read.parquet("s3://capstoneproject_Harshath/target/temp/")
transactions_dyf = glueContext.create_dynamic_frame.from_catalog(
    database="capstonedb",
    table_name="transactionsrc"
)
transactions_df=transactions_dyf.toDF()
geo_dyf = glueContext.create_dynamic_frame.from_catalog(
    database="capstonedb",
    table_name="geolocationsrc"
)
geolocation_df = geo_dyf.toDF()
join_cust_geo_df = customer_df.join(
    geolocation_df,
    customer_df.zip_code == geolocation_df.zip_code,
    "left"
)
sel_cust_geo_df
=join_cust_geo_df.select(customer_df.customer_id,customer_df.name,customer_df.email,customer_df.created_at,geolocation_df.city,
geolocation_df.state,customer_df.zip_code)
agg_trans_df = transactions_df.groupBy("customer_id").agg(count("customer_id").alias("transaction_count"),sum("transaction_amount").alias("total_transaction_amount")
)
join_cust_trans_df =
sel_cust_geo_df.join(agg_trans_df,sel_cust_geo_df.customer_id ==
agg_trans_df.customer_id,"left")
sel_cust_trans_df
=join_cust_trans_df.select(sel_cust_geo_df.customer_id,sel_cust_geo_df.name,sel_cust_geo_df.email,sel_cust_geo_df.created_at,sel_cust_geo_df.city,sel_cust_geo_df.state,sel_cust_geo_df.zip_code,agg_trans_df.transaction_count,agg_trans_df.total_transaction_amount)
cust_year_df = sel_cust_trans_df.withColumn("created_year",year(to_date("created_at","yyyy-MM-dd")))
cust_year_df.write.mode("overwrite").partitionBy("state","created_year").parquet("s3://capstarget/final_customer_360/")
job.commit()
